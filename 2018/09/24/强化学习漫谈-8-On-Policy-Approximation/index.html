<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="强化学习," />










<meta name="description" content="MathJax.Hub.Config({   TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } } });   离开Tabular方法，进入Approximation的世界。这一章先讲On-Policy部分。 On-Policy Prediction With Approximation用回归Regression的方法来近似值函数，这是强化学习中除了Ta">
<meta name="keywords" content="强化学习">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习漫谈 8：On-Policy with Approximation">
<meta property="og:url" content="http://yoursite.com/2018/09/24/强化学习漫谈-8-On-Policy-Approximation/index.html">
<meta property="og:site_name" content="About Learning">
<meta property="og:description" content="MathJax.Hub.Config({   TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } } });   离开Tabular方法，进入Approximation的世界。这一章先讲On-Policy部分。 On-Policy Prediction With Approximation用回归Regression的方法来近似值函数，这是强化学习中除了Ta">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-09-25T15:21:38.765Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="强化学习漫谈 8：On-Policy with Approximation">
<meta name="twitter:description" content="MathJax.Hub.Config({   TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } } });   离开Tabular方法，进入Approximation的世界。这一章先讲On-Policy部分。 On-Policy Prediction With Approximation用回归Regression的方法来近似值函数，这是强化学习中除了Ta">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/09/24/强化学习漫谈-8-On-Policy-Approximation/"/>





  <title>强化学习漫谈 8：On-Policy with Approximation | About Learning</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">About Learning</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/24/强化学习漫谈-8-On-Policy-Approximation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="刘志欣">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="About Learning">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">强化学习漫谈 8：On-Policy with Approximation</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-24T23:45:32+08:00">
                2018-09-24
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/09/24/强化学习漫谈-8-On-Policy-Approximation/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/09/24/强化学习漫谈-8-On-Policy-Approximation/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/09/24/强化学习漫谈-8-On-Policy-Approximation/" class="leancloud_visitors" data-flag-title="强化学习漫谈 8：On-Policy with Approximation">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<p>离开Tabular方法，进入Approximation的世界。这一章先讲On-Policy部分。</p>
<h3 id="On-Policy-Prediction-With-Approximation"><a href="#On-Policy-Prediction-With-Approximation" class="headerlink" title="On-Policy Prediction With Approximation"></a>On-Policy Prediction With Approximation</h3><p>用回归Regression的方法来近似值函数，这是强化学习中除了Tabular方法之外常见的方法。从状态空间抽象出特征构成特征空间，然后用特征作为输入，运用machine learning的方法进行回归。从machine learning的角度，我们自然会有如下的思路：</p>
<ol>
<li>特征空间比状态空间维度低很多，因此回归过程中参数的改变会改变很多或者所以状态的值函数。如何针对这一情况定义出loss function以确保状态空间中所有状态的值函数收敛到我们期望的目标？</li>
<li>定义出loss function后，针对训练样本，每一步的目标target是什么？</li>
<li>训练方法是什么？<a id="more"></a>
<h5 id="Loss-Function-The-Prediction-Objective"><a href="#Loss-Function-The-Prediction-Objective" class="headerlink" title="Loss Function: The Prediction Objective"></a>Loss Function: The Prediction Objective</h5>在Tabular方法中，因为每个状态的值更新是相对独立的，并且以最终达到或近似达到真实值函数为目标，所以不需要定义或者关注连续的prediction quality。但是在值函数近似方法中，因为通常都是用连续函数对特征空间进行回归，参数的影响是非孤立的甚至是全局的，因此我们在每一步的时候都要关注参数的调整对整体状态空间中的值函数的影响，同时也要考虑到状态的分布，对于出现概率高的状态，关注的权重自然会高一些。因此我们定义loss function为$\textit{Mean Squared Value Error}$ $\overline{VE}$如下：<br>\begin{equation}<br>\overline{VE}(w)=\sum_{s\in\mathcal{S}}\mu(s)[v_\pi(s)-\hat{v}(s,w)]^2<br>\end{equation}<br>其中$\mu(s)$为状态$s$的分布，$\sum_s\mu(s)=1$，$v_\pi(s)$为状态$s$在策略$\pi$下的值，$\hat{v}(s,w)$为回归函数，$w$为回归函数的参数集。</li>
</ol>
<p>对于$\textit{continuing tasks}$，状态分布$\mu(s)$为策略下状态的稳态分布；对于$\textit{episodic tasks}$，状态分布$\mu(s)$略微复杂一些，并且$\textbf{依赖于}$初始状态的分布。计算方式如下：<br>定义$h(s)$为episode初始状态为$s$的概率，$\eta(s)$为状态$s$在一个episode中出现次数的平均值，我们可以得到一个DP的表达：<br>\begin{equation}<br>\eta(s)=h(s)+\sum_{\overline{s}}\eta(\overline{s})\sum_a\pi(a|\overline{s})p(s|\overline{s},a), \forall s \in \mathcal{S}<br>\end{equation}<br>最终得到的on-policy的状态分布函数为：<br>\begin{equation}<br>\mu(s)=\frac{\eta(s)}{\sum_s\eta(s)}, \forall s \in \mathcal{S}<br>\end{equation}</p>
<p>定义了loss function $\overline{VE}$后，我们的目标就是寻找全局最优点$w^\ast$使得$\overline{VE}(w^\ast)\le \overline{VE}(w)$ for all possible $w$。但是在machine learning中始终会存在的问题在此处依然存在：非凸情况的优化，局部最优vs全局最优，等。</p>
<h5 id="每一步训练的Target"><a href="#每一步训练的Target" class="headerlink" title="每一步训练的Target"></a>每一步训练的Target</h5><p>顺着前几章的思路，我们很容易得出每一步训练的target：DP中，每一步的target是$v_\pi(S_t)$；MC中，每一步的target是return $G_t$；TD中，每一步的target是$R_{t+1}+\gamma V(S_{t+1})$；n-step TD中，每一步的target是$G_{t:t+n}$。在这里我们统一将每一步的目标记为$U_t$。</p>
<p>对于$U_t=G_t$，这个目标是无偏的；但是对于$U_t=G_{t:t+n}$，这个目标是有偏的，与当前的$w_t$有关，因此收敛性并不能确保；但是对于很多有用的case而言，例如线性回归，它的收敛性是有保障的（证明见下节线性回归部分）。而使用n-step TD的return为目标： $U_t=G_{t:t+n}$，它具有训练快，可在线训练（不需要等到episode结束）等优点，所以在实际中有一定的价值。</p>
<h5 id="训练方法：SGD-Semi-Gradient"><a href="#训练方法：SGD-Semi-Gradient" class="headerlink" title="训练方法：SGD, Semi-Gradient"></a>训练方法：SGD, Semi-Gradient</h5><p>在线的小步长训练可以借鉴SGD（Stochastic Gradient Descent）的方法：<br>\begin{equation}<br>w_{t+1}=w_t-\frac{1}{2}\alpha\nabla[U_t-\hat{v}(S_t,w_t)]^2<br>\end{equation}<br>当$U_t$与$w_t$无关时（例如，$U_t=G_t$，或者$U_t=v_\pi(S_t)$），上式可以写成：<br>\begin{equation}<br>w_{t+1}=w_t+\alpha[U_t-\hat{v}(S_t,w_t)]\nabla\hat{s}(S_t,w_t)<br>\end{equation}<br>但是当$U_t$与$w_t$相关时（例如$U_t=G_{t:t+n}$ for n-step TD case）,我们为了简便，任然沿用上式，称为Semi-Gradient Descent。<br>上式中$\alpha$为超参数learning rate。</p>
<p>同样，为了减少训练中的随机性，我们也可以使用mini-batch，具体不在详述。</p>
<h5 id="具体例子：线性回归"><a href="#具体例子：线性回归" class="headerlink" title="具体例子：线性回归"></a>具体例子：线性回归</h5><p>从状态空间到特征空间的映射定义为：$\textbf{x}(s)=(x_1(s),x_2(x),…,x_d(s))^\mathsf{T}$，$d$为特征空间的维度，在特征空间中用线性回归来近似值函数：<br>\begin{equation}<br>\hat{v}(s,w)=w^{\mathsf{T}}\textbf{x}(s)<br>\end{equation}<br>那么我们可以得到<br>\begin{equation}<br>\nabla\hat{v}(s,w)=\textbf{x}(s)<br>\end{equation}<br>代入到Semi-Gradient表达中：<br>\begin{equation}<br>w_{t+1}=w_t+\alpha[U_t-w_t^{\mathsf{T}}\textbf{x}(S_t)]\textbf{x}(S_t)<br>\end{equation}</p>
<p>对于TD(0)而言，$U_t=R_{t+1}+\gamma w_t^{\textsf{T}}\textbf{x}_{t+1}$我们可以进一步得到一些有趣的结论：<br>\begin{eqnarray}<br>w_{t+1}&amp;=&amp;w_t+\alpha(R_{t+1}+\gamma w_t^{\textsf{T}}\textbf{x}_{t+1}-w_t^{\textsf{T}}\textbf{x}_t)\textbf{x}_t\notag\\<br>&amp;=&amp;w_t+\alpha(R_{t+1}\textbf{x}_t-\textbf{x}_t(\textbf{x}_t-\gamma \textbf{x}_{t+1})^\textsf{T}w_t)\notag\\<br>\end{eqnarray}</p>
<p>当最终达到稳态时，$w_{t+1}=\mathbb{E}[w_{t+1}|w_t]$，所以：<br>\begin{eqnarray}<br>w_{t+1}&amp;=&amp;\mathbb{E}[w_{t+1}|w_t]\notag\\<br>&amp;=&amp;w_t+\alpha (\textbf{b}-\textbf{Aw}_t)\notag\\<br>&amp;=&amp;w_t\notag\\<br>\end{eqnarray}<br>所以$\textbf{b}-\textbf{Aw}_t=0$，其中 $\textbf{b}=\mathbb{E}[R_{t+1}\textbf{x}_t]$, $\textbf{A}=\mathbb{E}[\textbf{x}_t(\textbf{x}_t-\gamma \textbf{x}_{t+1})^\textsf{T}]$</p>
<p>因此在达到稳态时，$w_{TD}=\textbf{A}^{-1}\textbf{b}$，这个$w_{TD}$是最终的$w$收敛点，称为$\textit{TD fixed point}$。我们可以直接求解这个收敛点（这种方法称为Least-Squares TD），但是计算量偏大，所以对于TD(0)的线性回归，虽然存在理论解，还是倾向于使用Semi-Gradient的方法来迭代。</p>
<p>线性回归中，如何选取特征$\textbf{x}$？常用方法有多项式、傅里叶变换基函数、Coarse Coding（Source Coding里的思想）、Tile Coding（Coarse Coding的特例）、RBF等。</p>
<p>此外，除了线性回归外，非线性方法比如深度神经网络也越来越多的应用到Value Approximation中。</p>
<h5 id="TD-n-的进一步拓展：Interest-and-Emphasis"><a href="#TD-n-的进一步拓展：Interest-and-Emphasis" class="headerlink" title="TD(n)的进一步拓展：Interest and Emphasis"></a>TD(n)的进一步拓展：Interest and Emphasis</h5><p>用TD(n)的return作为目标$U_t$，如果我们想细化一些在不同的时间对不同的状态的关注度，我们可以引入两个概念：$I_t$是在时刻$t$对不同状态的关注度Interest，$M_t$是在时刻$t$对learning的重视程度emphasis，那么Semi-Gradient可以写成：<br>\begin{equation}<br>w_{t+n}=w_{t+n-1}+\alpha M_t[G_{t:t+n}-\hat{v}(S_t,w_t+n-1)]\nabla\hat{v}(S_t,w_{t+n-1})<br>\end{equation}<br>其中$M_t=I_t+\gamma^nM_{t-n}$。</p>
<h3 id="On-Policy-Control-with-Approximation"><a href="#On-Policy-Control-with-Approximation" class="headerlink" title="On-Policy Control with Approximation"></a>On-Policy Control with Approximation</h3><p>进入on-policy control with approximation，直观的想法就是用$q(s,a)$替换上面的$v(s)$，用Sarsa替换上面的TD。但是这种做法此处我们仅限episodic情况进行讨论。Continuing task的情况我们在第二节引入average rewards的设定重新考虑。</p>
<h5 id="Episodic任务的On-Policy-Control"><a href="#Episodic任务的On-Policy-Control" class="headerlink" title="Episodic任务的On-Policy Control"></a>Episodic任务的On-Policy Control</h5><p>快速带过。通用公式：<br>\begin{equation}<br>w_{t+1}=w_t+\alpha[U_t-\hat{q}(S_t,A_t,w_t)]\nabla\hat{q}(S_t,A_t,w_t)<br>\end{equation}</p>
<p>对于one-step Sarsa，套用Semi-Gradient我们有：<br>\begin{equation}<br>w_{t+1}=w_t+\alpha[R_{t+1}+\gamma \hat{q}(S_{t+1},A_{t+1},w_t)-\hat{q}(S_t,A_t,w_t)]\nabla\hat{q}(S_t,A_t,w_t)<br>\end{equation}<br>对于n-step Sarsa：<br>\begin{equation}<br>w_{t+n}=w_{t+n-1}+\alpha[G_{t:t+n}-\hat{q}(S_t,A_t,w_{t+n-1})]\nabla\hat{q}(S_t,A_t,w_{t+n-1})<br>\end{equation}<br>其中$G_{t:t+n}=R_{t+1}+\gamma R_{t+2}+…+\gamma^{n-1}R_{t+n}+\gamma^n\hat{q}(S_{t+n},A_{t+n},w_{t+n-1})$。对于Expected Sarsa也可以用类似的方法表达出来。</p>
<h5 id="Continuing-Task-Average-Reward"><a href="#Continuing-Task-Average-Reward" class="headerlink" title="Continuing Task: Average Reward"></a>Continuing Task: Average Reward</h5><p>Average Rewards具体介绍可以参见论文<a href="https://pdfs.semanticscholar.org/a7b8/3a8eda36a46fff93803e6ddefa1012279e05.pdf" target="_blank" rel="noopener">Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results</a>。<br>首先，我们之前关注的都是reward的积累效应（cumulative sum of rewards），包括discount设定。Discount设定有如下两个经典场景，一是在金融或类似场景中有利息interest的参数可以用discount ratio来描述；二是在infinite horizon也就是没有absorbing goal states (MDP 中以概率1进入本状态称为recurrent状态，而recurrent single-state classes通常称为absorbing states)的情况下，如果没有discount，那么累计reward会变得unbounded。</p>
<p>这两种情况之外，针对continuing task，如果有absorbing states，也就是finite horizon，那么<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.25.7353" target="_blank" rel="noopener">Kaelbling</a>证明了可以使用undiscounted 方法解决这类问题。</p>
<p>除了$\textit{cumulative}$ rewards方法外，另一种更加自然的对长远的优化目标进行学习和控制的方法(long-term measure of optimality)，称为$\textit{average}$ reward的方法，将在下面进行简要介绍。希望后面能有时间单独写一章这部分的读书笔记，包括Average Rewards的Bellman公式，DP, Value Iteration, Policy Iteration等。</p>
<p>1). Gain and Bias Optimality</p>
<p>Average reward MDP每一步的优化目标就是字面上的average reward per state，定义如下：<br>\begin{equation}<br>\rho^\pi(s)=\lim_{N\rightarrow\infty}\frac{\mathbb{E}(\sum_{t=0}^{N-1}R_t^\pi(s))}{N}<br>\end{equation}</p>
<p>对于unichain MDP而言，一个重要的性质就是average reward与状态无关，也就是<br>\begin{equation}<br>\rho^\pi(x)=\rho^\pi(y)=\rho^\pi=\sum_s\mu_\pi(s)\sum_a\pi(a|s)\sum_{s^\prime,r}p(s^\prime,r|s,a)r<br>\end{equation}</p>
<blockquote>
<blockquote>
<p>对于unichain MDP的average reward与状态无关，有一段如下的解释比较符合直观理解：The reason is that unichain policies generate a single recurrent class of states, and possibly a set of transient states. Since states in the recurrent class will be visited forever, the expected average reward connot differ across these states. Since the transient states will be eventually never be reentered, they can at most accumulate a finite total expected reward before entering a recurrent state, which vanishes under the limit.</p>
</blockquote>
</blockquote>
<p>回到state value的定义，如果我们只是简单地把reward相加做平均而不考虑discount，会面临unbounded的问题，因此我们需要定义一个新的state value，称为average adjusted sum of reward，一般都称为Bias Value或者Relative Value, 定义如下：<br>\begin{equation}<br>v_\pi(s)=\lim_{N\rightarrow\infty}\mathbb{E}(\sum_{t=0}^{N-1}(R_\pi(S_t)-\rho^\pi))<br>\end{equation}</p>
<p>2). Semi-Gradient for Average Reward<br>Return定义如下式：<br>\begin{equation}<br>G_t=R_{t+1}-\rho^\pi+R_{t+2}-\rho^\pi+…<br>\end{equation}<br>TD error定义如下：<br>\begin{equation}<br>\delta_t=R_{t+1}-\overline{R}_{t+1}+\hat{q}(S_{t+1},A_{t+1},w_t)-\hat{q}(S_t,A_t,w_t)<br>\end{equation}<br>其中$\overline{R}_t$是在时刻$t$的对average reward $\rho^\pi$的估计。<br>因此Semi-Gradient的表达为：<br>\begin{equation}<br>w_{t+1}=w_t+\alpha\delta_t\nabla\hat{q}(S_t,A_t,w_t)<br>\end{equation}</p>
<p>3). More about Average Reward<br><a href="http://www.mit.edu/~jnt/Papers/J088-02-bvr-av_disc.pdf" target="_blank" rel="noopener">这篇论文</a>讨论了从discount设定中$\gamma\rightarrow 1$的情况下，discount设定与average reward设定是如何统一的。而DeepMind的<a href="https://arxiv.org/pdf/1707.06887.pdf" target="_blank" rel="noopener">这篇论文</a>则不仅关注了reward的均值，还研究了reward的方差以及value distribution。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/强化学习/" rel="tag"># 强化学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/09/19/强化学习漫谈-7-剪枝与Planning/" rel="next" title="强化学习漫谈 7：剪枝与Planning">
                <i class="fa fa-chevron-left"></i> 强化学习漫谈 7：剪枝与Planning
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/09/27/强化学习漫谈-9-Off-Policy-Approximation/" rel="prev" title="强化学习漫谈 9：Off Policy Approximation">
                强化学习漫谈 9：Off Policy Approximation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">刘志欣</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#On-Policy-Prediction-With-Approximation"><span class="nav-number">1.</span> <span class="nav-text">On-Policy Prediction With Approximation</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Loss-Function-The-Prediction-Objective"><span class="nav-number">1.0.1.</span> <span class="nav-text">Loss Function: The Prediction Objective</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#每一步训练的Target"><span class="nav-number">1.0.2.</span> <span class="nav-text">每一步训练的Target</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#训练方法：SGD-Semi-Gradient"><span class="nav-number">1.0.3.</span> <span class="nav-text">训练方法：SGD, Semi-Gradient</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#具体例子：线性回归"><span class="nav-number">1.0.4.</span> <span class="nav-text">具体例子：线性回归</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TD-n-的进一步拓展：Interest-and-Emphasis"><span class="nav-number">1.0.5.</span> <span class="nav-text">TD(n)的进一步拓展：Interest and Emphasis</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#On-Policy-Control-with-Approximation"><span class="nav-number">2.</span> <span class="nav-text">On-Policy Control with Approximation</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Episodic任务的On-Policy-Control"><span class="nav-number">2.0.1.</span> <span class="nav-text">Episodic任务的On-Policy Control</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Continuing-Task-Average-Reward"><span class="nav-number">2.0.2.</span> <span class="nav-text">Continuing Task: Average Reward</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">刘志欣</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'iC82lD4bRxRuaSG490Jjc8yr-gzGzoHsz',
        appKey: 'MfUVQHRcRFwpnRUTSXCAMjiu',
        placeholder: 'Say Something',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("iC82lD4bRxRuaSG490Jjc8yr-gzGzoHsz", "MfUVQHRcRFwpnRUTSXCAMjiu");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
